[build-system]
requires = [
    "setuptools>=61.0",
    "wheel",
    "torch>=2.0.0; platform_machine != 'x86_64' or python_version < '3.8'",
    "pybind11>=2.10.0; platform_machine == 'x86_64'",
    "ninja; platform_machine == 'x86_64'",
]
build-backend = "setuptools.build_meta"

[project]
name = "vllm-lite"
version = "0.1.0"
description = "A lightweight, high-throughput LLM inference engine for consumer hardware"
readme = "README.md"
license = {text = "Apache-2.0"}
authors = [
    {name = "ved1beta", email = "ved1beta@example.com"}
]
maintainers = [
    {name = "ved1beta", email = "ved1beta@example.com"}
]
keywords = [
    "llm",
    "inference", 
    "attention",
    "cuda",
    "optimization",
    "machine-learning",
    "pytorch",
    "transformers"
]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research", 
    "License :: OSI Approved :: Apache Software License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: C++",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Operating System :: POSIX :: Linux",
    "Operating System :: Microsoft :: Windows",
    "Operating System :: MacOS",
]
requires-python = ">=3.8"

# Minimal core dependencies
dependencies = [
    "numpy>=1.21.0",
    "packaging>=20.0",
    "typing-extensions>=4.0.0",
    "psutil>=5.9.0",
]

# Optional dependency groups
[project.optional-dependencies]
# Core ML functionality
ml = [
    "torch>=2.0.0",
    "transformers>=4.30.0",
    "tokenizers>=0.13.0",
    "huggingface-hub>=0.15.0",
]

# Performance optimizations
perf = [
    "flash-attn>=2.0.0; platform_machine=='x86_64' and python_version>='3.8'",
    "xformers>=0.0.20; python_version>='3.8'",
    "accelerate>=0.20.0",
]

# Web serving capabilities
serve = [
    "fastapi>=0.95.0",
    "uvicorn[standard]>=0.22.0",
    "pydantic>=2.0.0",
]

# Quantization support
quant = [
    "bitsandbytes>=0.41.0",
    "auto-gptq>=0.4.0",
    "optimum>=1.12.0",
]

# Distributed computing
distributed = [
    "ray[default]>=2.5.0",
]

# Monitoring and metrics
monitor = [
    "pynvml>=11.0.0",
    "prometheus-client>=0.16.0",
    "tensorboard>=2.13.0",
]

# Development tools
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "pytest-benchmark>=4.0.0",
    "pytest-cov>=4.1.0",
    "black>=23.0.0",
    "isort>=5.12.0",
    "flake8>=6.0.0",
    "mypy>=1.4.0",
    "pre-commit>=3.3.0",
    "ruff>=0.0.280",
]

# Documentation
docs = [
    "sphinx>=7.0.0",
    "sphinx-rtd-theme>=1.2.0",
    "myst-parser>=2.0.0",
    "sphinx-autodoc-typehints>=1.23.0",
]

# Convenience combinations
standard = ["vllm-lite[ml,serve]"]
full = ["vllm-lite[ml,perf,serve,monitor]"]
all = ["vllm-lite[ml,perf,serve,quant,distributed,monitor,dev,docs]"]

[project.urls]
Homepage = "https://github.com/ved1beta/vllm-lite"
Documentation = "https://github.com/ved1beta/vllm-lite#readme"
Repository = "https://github.com/ved1beta/vllm-lite.git"
"Bug Reports" = "https://github.com/ved1beta/vllm-lite/issues"
Changelog = "https://github.com/ved1beta/vllm-lite/blob/main/CHANGELOG.md"

[project.scripts]
vllm-lite = "vllm_lite.entrypoints.cli:main"
vllm-lite-serve = "vllm_lite.entrypoints.api_server:main"
vllm-lite-benchmark = "vllm_lite.benchmarks.benchmark:main"

# Setuptools configuration
[tool.setuptools]
zip-safe = false
include-package-data = true

[tool.setuptools.packages.find]
exclude = ["tests*", "benchmarks*", "examples*"]

[tool.setuptools.package-data]
vllm_lite = [
    "py.typed",
    "configs/*.yaml", 
    "configs/*.json",
]

# Development tool configurations

[tool.black]
line-length = 88
target-version = ['py38', 'py39', 'py310', 'py311']
include = '\.pyi?$'
exclude = '''
/(
    \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | _build
  | buck-out
  | build
  | dist
  | csrc
)/
'''

[tool.isort]
profile = "black"
multi_line_output = 3
line_length = 88
known_first_party = ["vllm_lite"]
known_third_party = ["torch", "transformers", "numpy"]
skip = ["csrc", "build"]

[tool.mypy]
python_version = "3.8"
warn_return_any = true
warn_unused_configs = true
ignore_missing_imports = true
exclude = [
    "build/",
    "csrc/",
    "examples/",
]

[[tool.mypy.overrides]]
module = [
    "torch.*",
    "transformers.*", 
    "flash_attn.*",
    "xformers.*",
]
ignore_missing_imports = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "--strict-markers",
    "--strict-config",
    "--verbose",
    "--tb=short",
]
markers = [
    "slow: marks tests as slow",
    "gpu: marks tests requiring GPU",
    "integration: marks tests as integration tests",
]
filterwarnings = [
    "ignore::UserWarning",
    "ignore::DeprecationWarning",
]

[tool.coverage.run]
source = ["vllm_lite"]
omit = [
    "tests/*",
    "examples/*",
    "benchmarks/*",
    "vllm_lite/_C/*",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
]

[tool.ruff]
target-version = "py38"
line-length = 88
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
    "B",  # flake8-bugbear
    "C4", # flake8-comprehensions
    "UP", # pyupgrade
]
ignore = [
    "E501",  # line too long, handled by black
    "B008",  # do not perform function calls in argument defaults
    "C901",  # too complex
]
exclude = [
    ".bzr",
    ".direnv", 
    ".eggs",
    ".git",
    ".mypy_cache",
    ".pants.d",
    ".ruff_cache",
    ".svn",
    ".tox",
    ".venv",
    "__pypackages__",
    "_build",
    "buck-out",
    "build",
    "dist",
    "node_modules",
    "venv",
    "csrc",
]

[tool.ruff.per-file-ignores]
"__init__.py" = ["F401"]
"tests/*" = ["S101"]

# Environment-specific configurations
[tool.conda-lock]
channels = ["pytorch", "nvidia", "conda-forge"]
platforms = ["linux-64", "win-64", "osx-64", "osx-arm64"]

[tool.pip-tools]
generate-hashes = true
upgrade = true